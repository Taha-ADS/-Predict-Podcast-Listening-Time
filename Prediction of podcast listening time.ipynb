{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91715,"databundleVersionId":11351736,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-27T15:08:38.803735Z","iopub.execute_input":"2025-04-27T15:08:38.804021Z","iopub.status.idle":"2025-04-27T15:08:39.155795Z","shell.execute_reply.started":"2025-04-27T15:08:38.803993Z","shell.execute_reply":"2025-04-27T15:08:39.154912Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/playground-series-s5e4/sample_submission.csv\n/kaggle/input/playground-series-s5e4/train.csv\n/kaggle/input/playground-series-s5e4/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import RidgeCV\nimport numpy as np\nfrom scipy.sparse import hstack\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.metrics import mean_squared_error\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import RidgeCV, SGDRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T16:04:12.000710Z","iopub.execute_input":"2025-04-27T16:04:12.001056Z","iopub.status.idle":"2025-04-27T16:04:12.008068Z","shell.execute_reply.started":"2025-04-27T16:04:12.001031Z","shell.execute_reply":"2025-04-27T16:04:12.007110Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"try:\n    train = pd.read_csv('/kaggle/input/playground-series-s5e4/train.csv')\n    test = pd.read_csv('/kaggle/input/playground-series-s5e4/test.csv')\n    sample_submission_df = pd.read_csv('/kaggle/input/playground-series-s5e4/sample_submission.csv') # Also load sample submission for context\n\n    print(\"--- Datasets loaded successfully ---\")\n    print(\"\\n\")\n\n    # --- Display basic information about the training data ---\n    print(\"--- Training Data ---\")\n    print(\"Shape (rows, columns):\", train_df.shape)\n    print(\"\\nFirst 5 rows:\")\n    print(train_df.head())\n    print(\"\\nData types and non-null counts:\")\n    train_df.info()\n    print(\"-\" * 30)\n    print(\"\\n\")\n\n    # --- Display basic information about the test data ---\n    print(\"--- Test Data ---\")\n    print(\"Shape (rows, columns):\", test_df.shape)\n    print(\"\\nFirst 5 rows:\")\n    print(test_df.head())\n    print(\"\\nData types and non-null counts:\")\n    test_df.info()\n    print(\"-\" * 30)\n    print(\"\\n\")\n\n    # --- Display basic information about the sample submission ---\n    print(\"--- Sample Submission ---\")\n    print(\"Shape (rows, columns):\", sample_submission_df.shape)\n    print(\"\\nFirst 5 rows:\")\n    print(sample_submission_df.head())\n    print(\"\\nData types and non-null counts:\")\n    sample_submission_df.info()\n    print(\"-\" * 30)\n\nexcept FileNotFoundError as e:\n    print(f\"Error loading files: {e}\")\n    print(\"Please ensure 'train.csv', 'test.csv', and 'sample_submission.csv' are in the same directory as the script.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T15:24:16.087845Z","iopub.execute_input":"2025-04-27T15:24:16.088179Z","iopub.status.idle":"2025-04-27T15:24:18.112146Z","shell.execute_reply.started":"2025-04-27T15:24:16.088154Z","shell.execute_reply":"2025-04-27T15:24:18.111209Z"}},"outputs":[{"name":"stdout","text":"--- Datasets loaded successfully ---\n\n\n--- Training Data ---\nShape (rows, columns): (750000, 12)\n\nFirst 5 rows:\n   id     Podcast_Name Episode_Title  Episode_Length_minutes       Genre  \\\n0   0  Mystery Matters    Episode 98                     NaN  True Crime   \n1   1    Joke Junction    Episode 26                  119.80      Comedy   \n2   2   Study Sessions    Episode 16                   73.90   Education   \n3   3   Digital Digest    Episode 45                   67.17  Technology   \n4   4      Mind & Body    Episode 86                  110.51      Health   \n\n   Host_Popularity_percentage Publication_Day Publication_Time  \\\n0                       74.81        Thursday            Night   \n1                       66.95        Saturday        Afternoon   \n2                       69.97         Tuesday          Evening   \n3                       57.22          Monday          Morning   \n4                       80.07          Monday        Afternoon   \n\n   Guest_Popularity_percentage  Number_of_Ads Episode_Sentiment  \\\n0                          NaN            0.0          Positive   \n1                        75.95            2.0          Negative   \n2                         8.97            0.0          Negative   \n3                        78.70            2.0          Positive   \n4                        58.68            3.0           Neutral   \n\n   Listening_Time_minutes  \n0                31.41998  \n1                88.01241  \n2                44.92531  \n3                46.27824  \n4                75.61031  \n\nData types and non-null counts:\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 750000 entries, 0 to 749999\nData columns (total 12 columns):\n #   Column                       Non-Null Count   Dtype  \n---  ------                       --------------   -----  \n 0   id                           750000 non-null  int64  \n 1   Podcast_Name                 750000 non-null  object \n 2   Episode_Title                750000 non-null  object \n 3   Episode_Length_minutes       662907 non-null  float64\n 4   Genre                        750000 non-null  object \n 5   Host_Popularity_percentage   750000 non-null  float64\n 6   Publication_Day              750000 non-null  object \n 7   Publication_Time             750000 non-null  object \n 8   Guest_Popularity_percentage  603970 non-null  float64\n 9   Number_of_Ads                749999 non-null  float64\n 10  Episode_Sentiment            750000 non-null  object \n 11  Listening_Time_minutes       750000 non-null  float64\ndtypes: float64(5), int64(1), object(6)\nmemory usage: 68.7+ MB\n------------------------------\n\n\n--- Test Data ---\nShape (rows, columns): (250000, 11)\n\nFirst 5 rows:\n       id         Podcast_Name Episode_Title  Episode_Length_minutes  \\\n0  750000  Educational Nuggets    Episode 73                   78.96   \n1  750001          Sound Waves    Episode 23                   27.87   \n2  750002        Joke Junction    Episode 11                   69.10   \n3  750003        Comedy Corner    Episode 73                  115.39   \n4  750004         Life Lessons    Episode 50                   72.32   \n\n       Genre  Host_Popularity_percentage Publication_Day Publication_Time  \\\n0  Education                       38.11        Saturday          Evening   \n1      Music                       71.29          Sunday          Morning   \n2     Comedy                       67.89          Friday          Evening   \n3     Comedy                       23.40          Sunday          Morning   \n4  Lifestyle                       58.10       Wednesday          Morning   \n\n   Guest_Popularity_percentage  Number_of_Ads Episode_Sentiment  \n0                        53.33            1.0           Neutral  \n1                          NaN            0.0           Neutral  \n2                        97.51            0.0          Positive  \n3                        51.75            2.0          Positive  \n4                        11.30            2.0           Neutral  \n\nData types and non-null counts:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 250000 entries, 0 to 249999\nData columns (total 11 columns):\n #   Column                       Non-Null Count   Dtype  \n---  ------                       --------------   -----  \n 0   id                           250000 non-null  int64  \n 1   Podcast_Name                 250000 non-null  object \n 2   Episode_Title                250000 non-null  object \n 3   Episode_Length_minutes       221264 non-null  float64\n 4   Genre                        250000 non-null  object \n 5   Host_Popularity_percentage   250000 non-null  float64\n 6   Publication_Day              250000 non-null  object \n 7   Publication_Time             250000 non-null  object \n 8   Guest_Popularity_percentage  201168 non-null  float64\n 9   Number_of_Ads                250000 non-null  float64\n 10  Episode_Sentiment            250000 non-null  object \ndtypes: float64(4), int64(1), object(6)\nmemory usage: 21.0+ MB\n------------------------------\n\n\n--- Sample Submission ---\nShape (rows, columns): (250000, 2)\n\nFirst 5 rows:\n       id  Listening_Time_minutes\n0  750000                  45.437\n1  750001                  45.437\n2  750002                  45.437\n3  750003                  45.437\n4  750004                  45.437\n\nData types and non-null counts:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 250000 entries, 0 to 249999\nData columns (total 2 columns):\n #   Column                  Non-Null Count   Dtype  \n---  ------                  --------------   -----  \n 0   id                      250000 non-null  int64  \n 1   Listening_Time_minutes  250000 non-null  float64\ndtypes: float64(1), int64(1)\nmemory usage: 3.8 MB\n------------------------------\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"num_cols = [\n    'Episode_Length_minutes',\n    'Host_Popularity_percentage',\n    'Guest_Popularity_percentage',\n    'Number_of_Ads'\n]\n\n# 1. Correlations with the target\nprint(\"=== Numeric Feature Correlations with Listening_Time_minutes ===\")\nprint(train[num_cols + ['Listening_Time_minutes']].corr()['Listening_Time_minutes'].sort_values(ascending=False))\n\n# 2. Mean Listening Time by categorical feature\ncat_cols = ['Genre', 'Publication_Day', 'Publication_Time', 'Episode_Sentiment']\nfor col in cat_cols:\n    print(f\"\\n=== Top 5 {col} by Average Listening_Time ===\")\n    means = train.groupby(col)['Listening_Time_minutes'].mean().sort_values(ascending=False)\n    print(means.head(5))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T15:24:22.095471Z","iopub.execute_input":"2025-04-27T15:24:22.095808Z","iopub.status.idle":"2025-04-27T15:24:22.406021Z","shell.execute_reply.started":"2025-04-27T15:24:22.095784Z","shell.execute_reply":"2025-04-27T15:24:22.405274Z"}},"outputs":[{"name":"stdout","text":"=== Numeric Feature Correlations with Listening_Time_minutes ===\nListening_Time_minutes         1.000000\nEpisode_Length_minutes         0.916749\nHost_Popularity_percentage     0.050870\nGuest_Popularity_percentage   -0.016014\nNumber_of_Ads                 -0.118337\nName: Listening_Time_minutes, dtype: float64\n\n=== Top 5 Genre by Average Listening_Time ===\nGenre\nMusic         46.578394\nTrue Crime    46.042507\nHealth        45.741413\nEducation     45.736640\nTechnology    45.634749\nName: Listening_Time_minutes, dtype: float64\n\n=== Top 5 Publication_Day by Average Listening_Time ===\nPublication_Day\nTuesday      46.131411\nMonday       45.969630\nWednesday    45.807177\nSaturday     45.326775\nFriday       45.206591\nName: Listening_Time_minutes, dtype: float64\n\n=== Top 5 Publication_Time by Average Listening_Time ===\nPublication_Time\nNight        46.456655\nAfternoon    45.525603\nMorning      44.964415\nEvening      44.761567\nName: Listening_Time_minutes, dtype: float64\n\n=== Top 5 Episode_Sentiment by Average Listening_Time ===\nEpisode_Sentiment\nPositive    46.723815\nNeutral     45.499110\nNegative    44.096838\nName: Listening_Time_minutes, dtype: float64\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# 1. Prepare X and y\nX = train[['Episode_Length_minutes']].copy()\ny = train['Listening_Time_minutes']\n\n# 2. Impute any missing episode lengths with the median\nX['Episode_Length_minutes'].fillna(X['Episode_Length_minutes'].median(), inplace=True)\n\n# 3. Train/validation split (80/20)\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# 4. Fit a simple Linear Regression\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\n# 5. Predict & evaluate\npreds = lr.predict(X_valid)\nrmse = np.sqrt(mean_squared_error(y_valid, preds))\nprint(\"Validation RMSE (Episode_Length only):\", rmse)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T15:25:43.011967Z","iopub.execute_input":"2025-04-27T15:25:43.012297Z","iopub.status.idle":"2025-04-27T15:25:43.186922Z","shell.execute_reply.started":"2025-04-27T15:25:43.012271Z","shell.execute_reply":"2025-04-27T15:25:43.185925Z"}},"outputs":[{"name":"stdout","text":"Validation RMSE (Episode_Length only): 13.504102080390092\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/3722330492.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  X['Episode_Length_minutes'].fillna(X['Episode_Length_minutes'].median(), inplace=True)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"X = train[num_cols].copy()\ny = train['Listening_Time_minutes']\n\n# 2. Impute missing values with each column’s median\nfor col in num_cols:\n    X[col].fillna(X[col].median(), inplace=True)\n\n# 3. Train/validation split (80/20)\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# 4. Fit Linear Regression\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\n# 5. Predict & evaluate\npreds = lr.predict(X_valid)\nrmse = np.sqrt(mean_squared_error(y_valid, preds))\nprint(\"Validation RMSE (All numeric features):\", rmse)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T15:27:16.119483Z","iopub.execute_input":"2025-04-27T15:27:16.119852Z","iopub.status.idle":"2025-04-27T15:27:16.385064Z","shell.execute_reply.started":"2025-04-27T15:27:16.119831Z","shell.execute_reply":"2025-04-27T15:27:16.384203Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/1316226645.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  X[col].fillna(X[col].median(), inplace=True)\n","output_type":"stream"},{"name":"stdout","text":"Validation RMSE (All numeric features): 13.360688984602454\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"X_num = train[num_cols].copy()\nX_cat = train[cat_cols].copy()\ny = train['Listening_Time_minutes']\n\n# 2. Impute numeric missing values with median\nfor col in num_cols:\n    X_num[col].fillna(X_num[col].median(), inplace=True)\n\n# 3. One-hot encode categoricals (drop first to avoid collinearity)\nX_cat = pd.get_dummies(X_cat, drop_first=True)\n\n# 4. Combine\nX = pd.concat([X_num, X_cat], axis=1)\n\n# 5. Train/validation split (80/20)\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# 6. Fit Linear Regression\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\n# 7. Predict & evaluate\npreds = lr.predict(X_valid)\nrmse = np.sqrt(mean_squared_error(y_valid, preds))\nprint(\"Validation RMSE (Numeric + One-Hot Cats):\", rmse)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T15:28:14.288450Z","iopub.execute_input":"2025-04-27T15:28:14.288808Z","iopub.status.idle":"2025-04-27T15:28:15.926127Z","shell.execute_reply.started":"2025-04-27T15:28:14.288784Z","shell.execute_reply":"2025-04-27T15:28:15.925082Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/3436928066.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  X_num[col].fillna(X_num[col].median(), inplace=True)\n","output_type":"stream"},{"name":"stdout","text":"Validation RMSE (Numeric + One-Hot Cats): 13.34639777602109\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Define a grid of alphas\nalphas = [0.01, 0.1, 1.0, 10.0, 100.0]\n\n# Set up RidgeCV (5-fold internally, using negative MSE)\nridge_cv = RidgeCV(alphas=alphas, cv=5, scoring='neg_mean_squared_error')\nridge_cv.fit(X_train, y_train)\n\nprint(\"Best alpha found:\", ridge_cv.alpha_)\n\n# Evaluate on validation\npreds = ridge_cv.predict(X_valid)\nrmse_ridge = np.sqrt(mean_squared_error(y_valid, preds))\nprint(\"Validation RMSE (Ridge):\", rmse_ridge)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T15:29:26.638081Z","iopub.execute_input":"2025-04-27T15:29:26.638388Z","iopub.status.idle":"2025-04-27T15:29:31.885713Z","shell.execute_reply.started":"2025-04-27T15:29:26.638366Z","shell.execute_reply":"2025-04-27T15:29:31.884712Z"}},"outputs":[{"name":"stdout","text":"Best alpha found: 100.0\nValidation RMSE (Ridge): 13.346385279648436\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# 2. Initialize and fit HGB\nhgb = HistGradientBoostingRegressor(\n    max_iter=200,\n    learning_rate=0.1,\n    max_leaf_nodes=31,\n    random_state=42\n)\nhgb.fit(X_train, y_train)\n\n# 3. Predict & evaluate\npreds = hgb.predict(X_valid)\nrmse_hgb = np.sqrt(mean_squared_error(y_valid, preds))\nprint(\"Validation RMSE (HistGradientBoosting):\", rmse_hgb)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T15:30:38.666309Z","iopub.execute_input":"2025-04-27T15:30:38.666638Z","iopub.status.idle":"2025-04-27T15:30:48.078384Z","shell.execute_reply.started":"2025-04-27T15:30:38.666616Z","shell.execute_reply":"2025-04-27T15:30:48.077692Z"}},"outputs":[{"name":"stdout","text":"Validation RMSE (HistGradientBoosting): 13.051563326842889\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"df = train.copy()\n\n# 2. Feature engineering\n#   a) Episode number from title (e.g. \"Episode 98\" → 98)\ndf['Episode_Number'] = df['Episode_Title'].str.extract(r'(\\d+)').astype(float)\n\n#   b) Frequency encoding of Podcast_Name\nfreq = df['Podcast_Name'].value_counts(normalize=True)\ndf['Podcast_Name_freq'] = df['Podcast_Name'].map(freq)\n\n# 3. Prepare feature matrix\nnum_cols = [\n    'Episode_Length_minutes',\n    'Host_Popularity_percentage',\n    'Guest_Popularity_percentage',\n    'Number_of_Ads',\n    'Episode_Number',\n    'Podcast_Name_freq'\n]\n\ncat_cols = ['Genre', 'Publication_Day', 'Publication_Time', 'Episode_Sentiment']\n\n# 4. Impute numeric\nX_num = df[num_cols].copy()\nfor c in num_cols:\n    X_num[c].fillna(X_num[c].median(), inplace=True)\n\n# 5. One-hot encode categoricals\nX_cat = pd.get_dummies(df[cat_cols], drop_first=True)\n\n# 6. Combine X, and target y\nX = pd.concat([X_num, X_cat], axis=1)\ny = df['Listening_Time_minutes']\n\n# 7. Train/validation split\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# 8. Re-train HGB\nhgb = HistGradientBoostingRegressor(\n    max_iter=200,\n    learning_rate=0.1,\n    max_leaf_nodes=31,\n    random_state=42\n)\nhgb.fit(X_train, y_train)\n\n# 9. Evaluate\npreds = hgb.predict(X_valid)\nrmse = np.sqrt(mean_squared_error(y_valid, preds))\nprint(\"Validation RMSE (with Episode_Number & Podcast_Name_freq):\", rmse)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T15:51:31.508716Z","iopub.execute_input":"2025-04-27T15:51:31.509007Z","iopub.status.idle":"2025-04-27T15:51:45.364858Z","shell.execute_reply.started":"2025-04-27T15:51:31.508987Z","shell.execute_reply":"2025-04-27T15:51:45.364137Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/2606388716.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  X_num[c].fillna(X_num[c].median(), inplace=True)\n","output_type":"stream"},{"name":"stdout","text":"Validation RMSE (with Episode_Number & Podcast_Name_freq): 13.03726407248093\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"df = train.copy()\ndf['Episode_Number'] = df['Episode_Title'].str.extract(r'(\\d+)').astype(float)\nfreq = df['Podcast_Name'].value_counts(normalize=True)\ndf['Podcast_Name_freq'] = df['Podcast_Name'].map(freq)\n\nnum_cols = [\n    'Episode_Length_minutes',\n    'Host_Popularity_percentage',\n    'Guest_Popularity_percentage',\n    'Number_of_Ads',\n    'Episode_Number',\n    'Podcast_Name_freq'\n]\ncat_cols = ['Genre', 'Publication_Day', 'Publication_Time', 'Episode_Sentiment']\n\n# Impute numerics\nX_num = df[num_cols].copy()\nfor c in num_cols:\n    X_num[c].fillna(X_num[c].median(), inplace=True)\n\n# One-hot encode cats\nX_cat = pd.get_dummies(df[cat_cols], drop_first=True)\n\n# 2. TF-IDF on Episode_Title\ntfidf = TfidfVectorizer(max_features=200, ngram_range=(1,2))\nX_tfidf = tfidf.fit_transform(df['Episode_Title'])\n\n# 3. Combine all features into a sparse matrix\nX_sparse = hstack([\n    X_tfidf,\n    np.array(X_num),       # dense to sparse automatically\n    np.array(X_cat)\n])\n\ny = df['Listening_Time_minutes']\n\n# 4. Train/validation split\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X_sparse, y, test_size=0.2, random_state=42\n)\n\n# WARNING: this can blow up your memory if X is large!\nX_train_d = X_train.toarray()\nX_valid_d = X_valid.toarray()\n\nridge_cv = RidgeCV(alphas=alphas, cv=5, scoring='neg_mean_squared_error')\nridge_cv.fit(X_train_d, y_train)\nprint(\"Best alpha:\", ridge_cv.alpha_)\npreds = ridge_cv.predict(X_valid_d)\nrmse = np.sqrt(mean_squared_error(y_valid, preds))\nprint(\"RMSE (dense RidgeCV):\", rmse)\n\n# 6. Evaluate\npreds = ridge_cv.predict(X_valid)\nrmse = np.sqrt(mean_squared_error(y_valid, preds))\nprint(\"Validation RMSE (with TF-IDF on Title):\", rmse)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T16:00:20.181841Z","iopub.execute_input":"2025-04-27T16:00:20.182175Z","iopub.status.idle":"2025-04-27T16:01:01.784764Z","shell.execute_reply.started":"2025-04-27T16:00:20.182153Z","shell.execute_reply":"2025-04-27T16:01:01.784114Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/3076631079.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  X_num[c].fillna(X_num[c].median(), inplace=True)\n","output_type":"stream"},{"name":"stdout","text":"Best alpha: 1.0\nRMSE (dense RidgeCV): 13.338399524682325\nValidation RMSE (with TF-IDF on Title): 13.338399524682325\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\n\n# ─── 1) Rebuild engineered feature matrix (no TF-IDF here) ────────────────\ndf = train.copy()\ndf['Episode_Number'] = df['Episode_Title'].str.extract(r'(\\d+)').astype(float)\nfreq = df['Podcast_Name'].value_counts(normalize=True)\ndf['Podcast_Name_freq'] = df['Podcast_Name'].map(freq)\n\nnum_cols = [\n    'Episode_Length_minutes',\n    'Host_Popularity_percentage',\n    'Guest_Popularity_percentage',\n    'Number_of_Ads',\n    'Episode_Number',\n    'Podcast_Name_freq'\n]\ncat_cols = ['Genre', 'Publication_Day', 'Publication_Time', 'Episode_Sentiment']\n\n# Impute numerics\nX_num = df[num_cols].fillna(df[num_cols].median())\n\n# One-hot encode cats\nX_cat = pd.get_dummies(df[cat_cols], drop_first=True)\n\n# Final feature matrix and target\nX = pd.concat([X_num, X_cat], axis=1)\ny = df['Listening_Time_minutes']\n\n# Train/val split\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n\n# ─── 2) RandomizedSearch over HGB params ────────────────────────────────────\n\nhgb = HistGradientBoostingRegressor(random_state=42)\n\nparam_dist = {\n    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n    'max_iter': [100, 200, 300],\n    'max_leaf_nodes': [15, 31, 63],\n    'min_samples_leaf': [20, 50, 100],\n    'l2_regularization': [0.0, 0.1, 1.0]\n}\n\nsearch = RandomizedSearchCV(\n    hgb,\n    param_dist,\n    n_iter=20,\n    scoring='neg_mean_squared_error',\n    cv=3,\n    random_state=42,\n    n_jobs=-1\n)\nsearch.fit(X_train, y_train)\n\nprint(\"Best HGB params:\", search.best_params_)\n\n# Evaluate on held-out validation\nbest_hgb = search.best_estimator_\npreds = best_hgb.predict(X_valid)\nrmse = np.sqrt(mean_squared_error(y_valid, preds))\nprint(\"Validation RMSE (tuned HGB):\", rmse)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T16:20:31.452496Z","iopub.execute_input":"2025-04-27T16:20:31.452902Z","iopub.status.idle":"2025-04-27T16:28:19.766513Z","shell.execute_reply.started":"2025-04-27T16:20:31.452875Z","shell.execute_reply":"2025-04-27T16:28:19.765610Z"}},"outputs":[{"name":"stdout","text":"Best HGB params: {'min_samples_leaf': 100, 'max_leaf_nodes': 63, 'max_iter': 300, 'learning_rate': 0.1, 'l2_regularization': 1.0}\nValidation RMSE (tuned HGB): 12.972773875532505\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"from scipy.sparse import csr_matrix\n\ndf = train.copy()\n\n# a) Engineered tabular features (for HGB)\ndf['Episode_Number'] = df['Episode_Title'].str.extract(r'(\\d+)').astype(float)\nfreq = df['Podcast_Name'].value_counts(normalize=True)\ndf['Podcast_Name_freq'] = df['Podcast_Name'].map(freq)\n\nnum_cols = [\n    'Episode_Length_minutes',\n    'Host_Popularity_percentage',\n    'Guest_Popularity_percentage',\n    'Number_of_Ads',\n    'Episode_Number',\n    'Podcast_Name_freq'\n]\ncat_cols = ['Genre', 'Publication_Day', 'Publication_Time', 'Episode_Sentiment']\n\nX_tab = pd.concat([\n    df[num_cols].fillna(df[num_cols].median()),\n    pd.get_dummies(df[cat_cols], drop_first=True)\n], axis=1)\n\n# b) Sparse text features (for SGD)\ntfidf = TfidfVectorizer(max_features=200, ngram_range=(1,2))\nX_tfidf = tfidf.fit_transform(df['Episode_Title'])\n\n# c) Stack into one sparse matrix—cast tabular to float64 so scipy accepts it\nX_sparse = hstack([\n    X_tfidf,\n    csr_matrix(X_tab.values.astype(np.float64))\n])\n\ny = df['Listening_Time_minutes'].values\n\n# ─── 2) Train / validation split ────────────────────────────────────────────\nX_tab_train, X_tab_valid, X_sparse_train, X_sparse_valid, y_train, y_valid = train_test_split(\n    X_tab, X_sparse, y, test_size=0.2, random_state=42\n)\n\n# ─── 3) Level-1 models ────────────────────────────────────────────────────────\n\n# (a) Tuned HGB on tabular data\nhgb = HistGradientBoostingRegressor(\n    max_iter=300,\n    learning_rate=0.1,\n    max_leaf_nodes=63,\n    min_samples_leaf=100,\n    l2_regularization=1.0,\n    random_state=42\n)\nhgb.fit(X_tab_train, y_train)\npred_tab_train = hgb.predict(X_tab_train).reshape(-1, 1)\npred_tab_valid = hgb.predict(X_tab_valid).reshape(-1, 1)\n\n# (b) SGDRegressor on sparse data\nsgd = SGDRegressor(penalty='l2', alpha=0.01, random_state=42, max_iter=1000, tol=1e-3)\nsgd.fit(X_sparse_train, y_train)\npred_sp_train = sgd.predict(X_sparse_train).reshape(-1, 1)\npred_sp_valid = sgd.predict(X_sparse_valid).reshape(-1, 1)\n\n# ─── 4) Meta-model training ───────────────────────────────────────────────────\nX_meta_train = np.hstack([pred_tab_train, pred_sp_train])\nX_meta_valid = np.hstack([pred_tab_valid, pred_sp_valid])\n\nmeta = LinearRegression()\nmeta.fit(X_meta_train, y_train)\n\n# Final ensemble evaluation\npred_ens = meta.predict(X_meta_valid)\nrmse_ens = np.sqrt(mean_squared_error(y_valid, pred_ens))\nprint(\"Validation RMSE (Stacked Ensemble):\", rmse_ens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T16:54:22.830456Z","iopub.execute_input":"2025-04-27T16:54:22.830802Z","iopub.status.idle":"2025-04-27T16:55:16.293373Z","shell.execute_reply.started":"2025-04-27T16:54:22.830781Z","shell.execute_reply":"2025-04-27T16:55:16.292561Z"}},"outputs":[{"name":"stdout","text":"Validation RMSE (Stacked Ensemble): 12.97380209910441\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"train['Episode_Number'] = train['Episode_Title'].str.extract(r'(\\d+)').astype(float)\ntest ['Episode_Number'] = test ['Episode_Title'].str.extract(r'(\\d+)').astype(float)\n\n# b) Podcast_Name frequency (using train distribution; unknowns → 0)\nfreq = train['Podcast_Name'].value_counts(normalize=True)\ntrain['Podcast_Name_freq'] = train['Podcast_Name'].map(freq)\ntest ['Podcast_Name_freq'] = test ['Podcast_Name'].map(freq).fillna(0)\n\n# c) Define columns\nnum_cols = [\n    'Episode_Length_minutes',\n    'Host_Popularity_percentage',\n    'Guest_Popularity_percentage',\n    'Number_of_Ads',\n    'Episode_Number',\n    'Podcast_Name_freq'\n]\ncat_cols = ['Genre', 'Publication_Day', 'Publication_Time', 'Episode_Sentiment']\n\n# d) Impute numerics on train (medians) and apply same to test\nmedians = train[num_cols].median()\nX_train_num = train[num_cols].fillna(medians)\nX_test_num  = test [num_cols].fillna(medians)\n\n# e) One-hot encode categoricals (drop_first for consistency)\nX_train_cat = pd.get_dummies(train[cat_cols], drop_first=True)\n# Remember the dummy-column names\ncat_dummy_cols = X_train_cat.columns\n\n# For test, align to the same set of dummy columns\nX_test_cat = pd.get_dummies(test[cat_cols], drop_first=True)\nX_test_cat = X_test_cat.reindex(columns=cat_dummy_cols, fill_value=0)\n\n# f) Final feature matrices\nX_train = pd.concat([X_train_num, X_train_cat], axis=1)\nX_test  = pd.concat([X_test_num,  X_test_cat ], axis=1)\ny_train = train['Listening_Time_minutes']\n\n# ─── 3) Train Tuned HGB on Full Data ────────────────────────────────────────\nmodel = HistGradientBoostingRegressor(\n    max_iter=300,\n    learning_rate=0.1,\n    max_leaf_nodes=63,\n    min_samples_leaf=100,\n    l2_regularization=1.0,\n    random_state=42\n)\nmodel.fit(X_train, y_train)\n\n# ─── 4) Predict on Test and Save Submission ─────────────────────────────────\npreds = model.predict(X_test)\nsubmission = pd.DataFrame({\n    'id': test['id'],\n    'Listening_Time_minutes': preds\n})\nsubmission.to_csv('submission.csv', index=False)\nprint(\"➡️  submission.csv created with\", len(submission), \"rows\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T17:04:42.416965Z","iopub.execute_input":"2025-04-27T17:04:42.417319Z","iopub.status.idle":"2025-04-27T17:05:08.563071Z","shell.execute_reply.started":"2025-04-27T17:04:42.417298Z","shell.execute_reply":"2025-04-27T17:05:08.562122Z"}},"outputs":[{"name":"stdout","text":"➡️  submission.csv created with 250000 rows\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}